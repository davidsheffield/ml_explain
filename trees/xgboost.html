<!DOCTYPE html>
<html>
<head>
  <meta charset="UTF-8">
  <title>ML Explain - XGBoost</title>
  <link rel="stylesheet" href="../global.css">
  <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
  <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
</head>

<body>
  <div class="content-wrapper">
    <h1>XGBoost</h1>
    <p><a href="https://xgboost.readthedocs.io/en/latest/tutorials/model.html">Introduction to boosted trees</a>.</p>
    <h2>Example 1</h2>
    <p>Simple data. Parameters:</p>
    <table>
      <thead>
        <tr>
          <td>Parameter</td>
          <td>Value</td>
        </tr>
      </thead>
      <tbody>
        <tr>
          <td>max_depth</td>
          <td>2</td>
        </tr>
        <tr>
          <td>num_boost_round</td>
          <td>3</td>
        </tr>
        <tr>
          <td>objective</td>
          <td>binary:logistic</td>
        </tr>
        <tr>
          <td>eta</td>
          <td>0.1</td>
        </tr>
        <tr>
          <td>lambda</td>
          <td>1</td>
        </tr>
        <tr>
          <td>gamma</td>
          <td>0</td>
        </tr>
        <tr>
          <td>min_child_weight</td>
          <td>0</td>
        </tr>
      </tbody>
    </table>
    <p>Data:</p>
    <table>
      <thead>
        <tr>
          <td rowspan="2">ID</td>
          <td>X</td>
          <td rowspan="2">y</td>
        </tr>
        <tr>
          <td>f0</td>
        </tr>
      </thead>
      <tbody>
        <tr>
          <td>A</td>
          <td>0</td>
          <td>0</td>
        </tr>
        <tr>
          <td>B</td>
          <td>1</td>
          <td>1</td>
        </tr>
      </tbody>
    </table>
    <p>The loss function for binary:logistic is
      $$L = \sum_i [y_i \log p_i + (1 - y_i) \log (1 - p_i)],$$
      where \(i\) runs over the observations in the data, \(y_i\) is the class of the observation, and \(p_i\) is the predicted probability. For binary:logistic, the probability
      $$p_i = \frac{1}{1 + \exp(-\hat{y}_i)},$$
      where \(\hat{y}_i\) is the prediction from the leaves of the trees.</p>
    <p>The gradient of the loss function
      $$\frac{\partial L}{\partial \hat{y}_i} = y_i - p_i.$$
      The diagonal of the Hessian
      $$\frac{\partial^2 L}{\partial \hat{y}_i^2} = -p_i (1 - p_i).$$
      XGBoost uses the sum over a node of the gradient and Hessian diagonal. If \(I_j\) is the set of data in leaf \(j\),
      $$G_j = \sum_{i \in I_j} (y_i - p_i)$$
      and
      $$H_j = -\sum_{i \in I_j} p_i (1 - p_i).$$</p>
    <p>XGBoost will first search for where to split the data in the feature <code>f0</code>. The split with divide the previous node into a left and right portion. XGBoost will attempt to maximize the gain
      $$\text{Gain} = \frac{G_L^2}{\lambda - H_L} + \frac{G_R^2}{\lambda - H_R} - \frac{(G_L + G_R)^2}{\lambda - H_L - H_R},$$
      where \(\lambda\) is the regularization parameters <code>lambda</code>. By default, the starting probability for each class is 0.5. If the split point is greater than 1, both observations are split to the left; if the split is between 0 and 1, one observation is put in each node; and if the split point is less than 0, two observations are put on the right. The gains in the table below show that the best split is in the middle: 0.5.</p>
    <table>
      <thead>
        <tr>
          <td>Split</td>
          <td><code>f0</code> &lt; 0</td>
          <td><code>f0</code> &lt; 0.5</td>
          <td><code>f0</code> &lt; 1.1</td>
        </tr>
      </thead>
      <tbody>
        <tr>
          <td>\(G_L\)</td>
          <td>0</td>
          <td>0.5</td>
          <td>0</td>
        </tr>
        <tr>
          <td>\(G_R\)</td>
          <td>0</td>
          <td>-0.5</td>
          <td>0</td>
        </tr>
        <tr>
          <td>\(H_L\)</td>
          <td>0.5</td>
          <td>0.25</td>
          <td>0.5</td>
        </tr>
        <tr>
          <td>\(H_R\)</td>
          <td>0.5</td>
          <td>0.25</td>
          <td>0.5</td>
        </tr>
        <tr>
          <td>Gain</td>
          <td>0</td>
          <td>0.4</td>
          <td>0</td>
        </tr>
      </tbody>
    </table>
    <p>Whether or not the node is split depends on other parameters. The gain must be at least as large as <code>gamma</code>. Each child node's \(H\) must be at least as large as <code>min_child_weight</code>.</p>
    <p>The weight for a leaf with learning rate \(\eta\) is
      $$w_j = -\eta \frac{G_j}{\lambda - H_j}.$$</p>
    <h3>Tree 0</h3>
    <svg width="220" height="300">
        <rect x="60" y="0" width="100" height="40" fill="#999999"></rect>
        <text x="60" y="40">f0 &lt; 0.5</text>
        <line x1="110" y1="40" x2="50" y2="80" style="stroke:#999999;stroke-width:2"></line>
        <text x="40" y="65">yes</text>
        <line x1="110" y1="40" x2="170" y2="80" style="stroke:#999999;stroke-width:2"></line>
        <text x="160" y="65">no</text>
        <rect x="0" y="80" width="100" height="40" fill="#999999"></rect>
        <text x="0" y="100">-0.04</text>
        <text x="0" y="120">A</text>
        <rect x="120" y="80" width="100" height="40" fill="#999999"></rect>
        <text x="120" y="100">0.04</text>
        <text x="120" y="120">B</text>
    </svg>

    <!--<p>The gradient of the loss function
      $$p_i = \frac{\mathrm{e}^{\hat{y}_i}}{\sum_j \mathrm{e}^{\hat{y}_j}}$$
      $$\frac{\partial p_i}{\partial \hat{y}_i} = \frac{\mathrm{e}^{\hat{y}_i}}{\sum_j \mathrm{e}^{\hat{y}_j}} - \left(\frac{\mathrm{e}^{\hat{y}_i}}{\sum_j \mathrm{e}^{\hat{y}_j}}\right)^2 = p_i (1 - p_i)$$
      $$\frac{\partial p_i}{\partial \hat{y}_k} = -\frac{\mathrm{e}^{\hat{y}_i} \mathrm{e}^{\hat{y}_k}}{(\sum_j \mathrm{e}^{\hat{y}_j})^2} = -p_i p_k, i \neq k$$
      $$L = \sum_i y_i \log p_i$$
      $$\frac{\partial L}{\partial \hat{y}_i} = y_i - p_i.$$
      The diagonal of the Hessian
      $$\frac{\partial^2 L}{\partial \hat{y}_i^2} = -p_i (1 - p_i).$$</p>
  <p>$$\frac{e^{y_0}}{e^{y_0} + e^{y_1} + e^{y_2}}$$</p>-->
  </div>
</body>

</html>
