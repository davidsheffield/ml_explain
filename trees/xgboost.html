<!DOCTYPE html>
<html>
<head>
  <meta charset="UTF-8">
  <title>ML Explain - XGBoost</title>
  <link rel="stylesheet" href="../global.css">
  <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
  <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
  <script src="https://d3js.org/d3.v5.min.js"></script>
  <script>
    // Prediction plot
    var pred_plot = function (id, data, pred) {
      var margin = {top: 50, right: 50, bottom: 50, left: 50}
      , width = 400 - margin.left - margin.right
      , height = 300 - margin.top - margin.bottom;

      var xScale = d3.scaleLinear()
      .domain([-1, 3])
      .range([0, width]);
      var yScale = d3.scaleLinear()
      .domain([-1, 3])
      .range([height, 0]);

      var svg = d3.select(id)
      .append("svg")
      .attr("width", width + margin.left + margin.right)
      .attr("height", height + margin.top + margin.bottom)
      .append("g")
      .attr("transform", "translate(" + margin.left + "," + margin.top + ")");

      // Add axes
      svg.append("g")
      .attr("class", "x axis")
      .attr("transform", "translate(0," + height + ")")
      .call(d3.axisBottom(xScale));
      svg.append("g")
      .attr("class", "y axis")
      .call(d3.axisLeft(yScale));

      // Training data
      var data_point = svg.selectAll(".data_point")
      .data(data)
      .enter().append("g")
      data_point.append("circle")
      .attr("class", "data_point")
      .attr("r", 5)
      .attr("cx", function(d) { return xScale(d.x); })
      .attr("cy", function(d) { return yScale(d.y); })
      data_point.append("text")
      .text(function(d) { return d.ID; })
      .attr("x", function(d) { return xScale(d.x); })
      .attr("dx", -5)
      .attr("y", function(d) { return yScale(d.y); })
      .attr("dy", -10);

      // Prediction
      svg.append("path")
      .datum(pred)
      .attr("fill", "none")
      .attr("stroke", "#dd3333")
      .attr("stroke-width", 1.5)
      .attr("d", d3.line()
      .x(function(d) { return xScale(d.x) })
      .y(function(d) { return yScale(d.y) })
      );
    }

    // Tree plot
    var tree_plot = function (id, data) {
      var margin = {top: 50, right: 50, bottom: 50, left: 50}
      , width = 600 - margin.left - margin.right
      , height = 300 - margin.top - margin.bottom;

      var svg = d3.select(id)
      .append("svg")
      .attr("width", width + margin.left + margin.right)
      .attr("height", height + margin.top + margin.bottom)
      .append("g")
      .attr("transform", "translate(" + margin.left + "," + margin.top + ")");

      var treemap = d3.tree().size([width, height]);
      var nodes = d3.hierarchy(data);
      nodes = treemap(nodes);
      var links = nodes.descendants().slice(1);

      // Nodes
      var node = svg.selectAll(".node")
      .data(nodes.descendants())
      .enter().append("g")
      node.append("circle")
      .attr("class", "node")
      .attr("r", 4.5)
      .attr("cx", function(d) { return d.x; })
      .attr("cy", function(d) { return d.y; });
      node.append("text")
      .text(function(d) {
              if (d.data.leaf) {
                return "w = " + d.data.weight;
              } else {
                return d.data.condition;
              }
            })
      .attr("x", function(d) { return d.x; })
      .attr("dx", 5)
      .attr("y", function(d) { return d.y; });
      node.append("text")
      .text(function(d) {
              var contents = "";
              for (i = 0; i < d.data.observations.length; ++i) {
                contents += d.data.observations[i] + " ";
              }
              return contents;
            })
      .attr("x", function(d) { return d.x; })
      .attr("dx", 5)
      .attr("y", function(d) { return d.y; })
      .attr("dy", 15);
      node.append("text")
      .text(function(d) {
              if (d.data.leaf) {
                return "g = " + d.data.gradient;
              } else {
                return "gain = " + d.data.gain;
              }
            })
      .attr("x", function(d) { return d.x; })
      .attr("dx", 5)
      .attr("y", function(d) { return d.y; })
      .attr("dy", 30);
      node.append("text")
      .text(function(d) {
              if (d.data.leaf) {
                return "h = " + d.data.Hessian;
              } else {
                return "cover = " + d.data.cover;
              }
            })
      .attr("x", function(d) { return d.x; })
      .attr("dx", 5)
      .attr("y", function(d) { return d.y; })
      .attr("dy", 45);

      // Links
      svg.selectAll(".link")
      .data(links)
      .enter().append("path")
      .attr("class", "link")
      .attr("d", function(d) {
      return "M" + d.x + "," + d.y
      + "C" + d.x + "," + (d.y + d.parent.y) / 2
      + " " + d.parent.x + "," +  (d.y + d.parent.y) / 2
      + " " + d.parent.x + "," + d.parent.y;
      });
    }
  </script>
</head>

<body>
  <div class="content-wrapper">
    <h1>XGBoost</h1>
    <p><a href="https://github.com/dmlc/xgboost">XGBoost</a>'s algorithm is based on boosted decision trees. Each new tree learns from the training data relative to the previous trees' predictions. For the full derivation see the <a href="http://doi.acm.org/10.1145/2939672.2939785">original paper</a>.</p>
    <h2>Algorithm</h2>
    <p>The training data \(\{\mathbf{x}_i\}\) consit of \(m\) features and \(n\) observations with labels \(\{y_i\}\). We build \(K\) trees based on the training data and labels. The output of tree \(k\) and observation \(\mathbf{x}_i\) is \(f_k(\mathbf{x}_i)\). The prediction from the tree ensemble
    $$\hat{y}_i = \sum_{k=1}^K f_k(\mathbf{x}_i).$$
    This prediction may be further transformed for some objective functions such as classification.</p>
    <p>The prediction from the \(t\)th tree \(\hat{y}_i^{(t)} = \hat{y}_i^{(t-1)} + f_t(\mathbf{x}_i)\). The goal of the algorithm is to determine \(f_t(\mathbf{x}_i)\) to improve the prediction in terms of the objective function
    $$\mathcal{L}^{(t)} = \sum_{i=1}^n l(y_i, \hat{y}_i^{(t)}) + \Omega(f_t).$$
    The loss function \(l(y_i, \hat{y}_i)\) determines the effectiveness of \(\hat{y}_i\) predicting \(y_i\) and \(\Omega(f_t)\) is the regularization term. The parameter <code>objective</code> determines the loss function used as well as the transformation of \(\hat{y}_i\) to the prediction. The regularization
    $$\Omega(f_t) = \gamma T + \frac{\lambda}{2} \sum_{j=1}^T w_j^2,$$
    where \(\gamma\) is set by the parameter <code>gamma</code>, \(T\) is the number of leaves on the tree, \(w_j\) are the weights of the leaves, and \(\lambda\) (set by parameter <code>lambda</code>) is the L2 regularization parameter.</p>
    <p>The objective function is approximated by the Taylor expansion
    $$\mathcal{L}^{(t)} = \sum_{i=1}^n \left[l(y_i, \hat{y}_i^{(t - 1)}) + g_i f_t(\mathbf{x}_i) + \frac{1}{2} h_i f_t(\mathbf{x}_i)^2\right] + \Omega(f_t)$$
    for gradient
    $$g_i = \left.\frac{\partial l(y_i, \hat{y})}{\partial \hat{y}}\right|_{\hat{y}_i = \hat{y}_i^{(t-1)}}$$
    and Hessian
    $$h_i = \left.\frac{\partial^2 l(y_i, \hat{y})}{\partial \hat{y}^2}\right|_{\hat{y}_i = \hat{y}_i^{(t-1)}}.$$
    The optimal weight of leaf \(j\)
    $$w_j^* = -\frac{\sum_{i \in I_j} g_i}{\sum_{i \in I_j} h_i + \lambda},$$
    where \(I_j\) is the set of observations \(\mathbf{x}_i\) in leaf \(j\). The reduction of the objective function by a given split into left and right nodes is
    $$\frac{1}{2} \left[\frac{(\sum_{i \in I_L} g_i)^2}{\sum_{i \in I_L} h_i + \lambda} + \frac{(\sum_{i \in I_R} g_i)^2}{\sum_{i \in I_R} h_i + \lambda} - \frac{(\sum_{i \in I_L \cup I_R} g_i)^2}{\sum_{i \in I_L \cup I_R} h_i + \lambda}\right] - \gamma.$$</p>
    <p>XGBoost builds trees in a greedy manner. It begins with an initial prediction of 0.5 by default (set by the parameter <code>base_score</code>). It chooses the split (both feature and value) that produces the maximum gain. Each node is split futher as long as the gain is positive, the Hessian of the child node is below the parameter <code>min_child_weight</code>, or the tree level <code>max_depth</code> is reached. The next tree is then built based on that and all the previous trees.</p>
    <h2>Squared Error</h2>
    <p>The loss function for <code>reg:squarederror</code>
    $$l(y_i, \hat{y}_i) = \frac{1}{2}(y_i - \hat{y}_i)^2.$$
    The gradient
    $$g_i(y_i, \hat{y}_i) = \hat{y}_i - y_i$$
    and Hessian
    $$h_i(y_i, \hat{y}_i) = 1.$$</p>
    <p>The parameters used in this example:</p>
    <table>
      <thead>
        <tr>
          <td>Parameter</td>
          <td>Value</td>
        </tr>
      </thead>
      <tbody>
        <tr>
          <td>max_depth</td>
          <td>2</td>
        </tr>
        <tr>
          <td>num_boost_round</td>
          <td>3</td>
        </tr>
        <tr>
          <td>objective</td>
          <td>reg:squarederror</td>
        </tr>
        <tr>
          <td>eta</td>
          <td>0.5</td>
        </tr>
        <tr>
          <td>lambda</td>
          <td>1</td>
        </tr>
        <tr>
          <td>gamma</td>
          <td>0</td>
        </tr>
        <tr>
          <td>min_child_weight</td>
          <td>0</td>
        </tr>
      </tbody>
    </table>
    <p>The training data for the example:</p>
    <table>
      <thead>
        <tr>
          <td rowspan="2">ID</td>
          <td>X</td>
          <td rowspan="2">y</td>
        </tr>
        <tr>
          <td>f0</td>
        </tr>
      </thead>
      <tbody>
        <tr>
          <td>A</td>
          <td>0</td>
          <td>0</td>
        </tr>
        <tr>
          <td>B</td>
          <td>1</td>
          <td>1</td>
        </tr>
        <tr>
          <td>C</td>
          <td>2</td>
          <td>2</td>
        </tr>
      </tbody>
    </table>
    <p>Unless otherwise specified by the parameter <code>base_score</code>, the prediction begins at 0.5.</p>
    <div id="mse_initial_pred"></div>
    <script>
      var data_mse = [{"ID": "A", "x": 0, "y": 0},
                      {"ID": "B", "x": 1, "y": 1},
                      {"ID": "C", "x": 2, "y": 2}]
      var pred = [{"x": -1, "y": 0.5}, {"x": 3, "y": 0.5}]
      pred_plot("#mse_initial_pred", data_mse, pred)
    </script>
    <h3>Tree 0</h3>
    <p>XGBoost sorts the features and searches for the split with the most gain, where
    $$\text{Gain} = \frac{(\sum_{i \in I_L} g_i)^2}{\sum_{i \in I_L} h_i + \lambda} + \frac{(\sum_{i \in I_R} g_i)^2}{\sum_{i \in I_R} h_i + \lambda} - \frac{(\sum_{i \in I_L \cup I_R} g_i)^2}{\sum_{i \in I_L \cup I_R} h_i + \lambda}.$$
    The split with the most gain is to separate A from B and C.</p>
    <table>
      <thead>
        <tr>
          <td>Condition</td>
          <td>Left</td>
          <td>Right</td>
          <td>Grad Left</td>
          <td>Grad Right</td>
          <td>Hessian Left</td>
          <td>Hessian Right</td>
          <td>Gain</td>
        </tr>
      </thead>
      <tbody>
        <tr>
          <td>f0 &lt; 0</td>
          <td>&nbsp;</td>  
          <td>ABC</td>
          <td>0</td>
          <td>0.5 - 0.5 - 1.5 = -1.5</td>
          <td>0</td>
          <td>1 + 1 + 1 = 3</td>
          <td>0</td>
        </tr>
        <tr>
          <td>f0 &lt; 0.5</td>
          <td>A</td>
          <td>BC</td>
          <td>0.5 = 0.5</td>
          <td>-0.5 - 1.5 = -2</td>
          <td>1 = 1</td>
          <td>1 + 1 = 2</td>
          <td>0.896</td>
        </tr>
        <tr>
          <td>f0 &lt; 1.5</td>
          <td>AB</td>
          <td>C</td>
          <td>0.5 - 0.5 = 0</td>
          <td>-1.5 = -1.5</td>
          <td>1 + 1 = 2</td>
          <td>1 = 1</td>
          <td>0.563</td>
        </tr>
        <tr>
          <td>f0 &lt; 2.5</td>
          <td>ABC</td>
          <td>&nbsp;</td>
          <td>0.5 - 0.5 - 1.5 = -1.5</td>
          <td>0</td>
          <td>1 + 1 + 1 = 3</td>
          <td>0</td>
          <td>0</td>
        </tr>
      </tbody>
    </table>
    <p>The weights are
    $$w_L = -\eta \frac{G_L}{H_L + \lambda} = -0.5 \frac{0.5}{1 + 1} = -0.125$$
    and
    $$w_R = -\eta \frac{G_R}{H_R + \lambda} = -0.5 \frac{-2}{2 + 1} = 0.333.$$</p>
    <div id="mse_tree0"></div>
    <script>
      var data = {
        "condition": "f0 < 0.5",
        "leaf": false,
        "observations": ["A", "B", "C"],
        "gain": 0.895833373,
        "cover": 3,          
        "children": [
          {"leaf": true,
           "name": "A",
           "direction": "yes",
           "observations": ["A"],
           "gradient": 0.5,
           "Hessian": 1,
           "weight": -0.125,
           "cover":1},
          {"leaf": true,
           "name": "BC",
           "direction": "no",
           "observations": ["B", "C"],
           "gradient": -2,
           "Hessian": 2,
           "weight": 0.333333343,
           "cover": 2}]};
      tree_plot("#mse_tree0", data);
    </script>
    <p>The gain for splitting B and C is -0.0833. This means a split would make the objective function worse, so the initial tree is complete. To get the prediction, we must add it to the starting prediction of 0.5. So below 0.5, we prediction 0.5 - 0.125 = 0.375 and above it, we predict 0.833.</p>
    <div id="mse_tree0_pred"></div>
    <script>
      var pred = [{"x": -1, "y": 0.375}, {"x": 0.5, "y": 0.375}, {"x": 0.5, "y": 0.8333333}, {"x": 3, "y": 0.8333333}]
      pred_plot("#mse_tree0_pred", data_mse, pred)
    </script>
    <h3>Tree 1</h3>
    <p>The split with the most gain separates A and B from C. The gain is 0.465, while splitting A from B and C is 0.433. The algorithm proceeds by splitting A from B with a gain of 0.0697.</p>
    <div id="mse_tree1"></div>
    <script>
      var data = {
        "leaf": false,
        "condition": "f0 < 1.5",
        "observations": ["A", "B", "C"],
        "gain": 0.465422451,
        "cover": 3,
        "children": [
          {"leaf": false,
           "condition": "f0 < 0.5",
           "direction": "yes",
           "observations": ["A", "B"],
           "gradient": 0.208333333,
           "Hessian": 2,
           "gain": 0.0697337836,
           "cover": 2,
           "children": [
             {"leaf": true,
              "name": "A",
              "direction": "yes",
              "observations": ["A"],
              "gradient": 0.375,
              "Hessian": 1,
              "weight": -0.09375,
              "cover": 1},
             {"leaf": true,
              "name": "B",
              "direction": "no",
              "observations": ["B"],
              "gradient": -0.166666667,
              "Hessian": 1,
              "weight": 0.0416666567,
              "cover": 1}]},
          {"leaf": true,
           "name": "C",
           "direction": "no",
           "observations": ["C"],
           "gradient": -1.166666667,
           "Hessian": 1,
           "weight": 0.291666657,
           "cover": 1}]};
      tree_plot("#mse_tree1", data);
    </script>
    <div id="mse_tree1_pred"></div>
    <script>
      var pred = [{"x": -1, "y": 0.28125}, {"x": 0.5, "y": 0.28125}, {"x": 0.5, "y": 0.875}, {"x": 1.5, "y": 0.875}, {"x": 1.5, "y": 1.125}, {"x": 3, "y": 1.125}]
      pred_plot("#mse_tree1_pred", data_mse, pred)
    </script>
    <h3>Tree 2</h3>
    <p>The split with the most gain separates A and B from C again. The gain is 0.262, while splitting A from B and C is 0.244. The algorithm proceeds by splitting A from B with a gain of 0.0392.</p>
    <div id="mse_tree2"></div>
    <script>
      var data = {
        "leaf": false,
        "condition": "f0 < 1.5",
        "observations": ["A", "B"],
        "gain": 0.26180014,
        "cover": 3,
        "children": [
          {"leaf": false,
           "condition": "f0 < 0.5",
           "direction": "yes",
           "observations": ["A", "B"],
           "gradient": 0.15625,
           "Hessian": 2,
           "gain": 0.0392252617,
           "cover": 2,
           "children": [
             {"leaf": true,
              "name": "A",
              "observations": ["A"],
              "direction": "yes",
              "gradient": 0.28125,
              "Hessian": 1,
              "weight": -0.0703125,
              "cover": 1},
             {"leaf": true,
              "name": "B",
              "observations": ["B"],
              "direction": "no",
              "gradient": -0.125,
              "Hessian": 1,
              "weight": 0.03125,
              "cover": 1}]},
          {"leaf": true,
           "name": "C",
           "observations": ["C"],
           "direction": "no",
           "gradient": -0.875,
           "Hessian": 1,
           "weight": 0.21875,
           "cover": 1}]};
      tree_plot("#mse_tree2", data);
    </script>
    <div id="mse_tree2_pred"></div>
    <script>
      var pred = [{"x": -1, "y": 0.2109375}, {"x": 0.5, "y": 0.2109375}, {"x": 0.5, "y": 0.90625}, {"x": 1.5, "y": 0.90625}, {"x": 1.5, "y": 1.34375}, {"x": 3, "y": 1.34375}]
      pred_plot("#mse_tree2_pred", data_mse, pred)
    </script>
  </div>
</body>

</html>
